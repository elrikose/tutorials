# Simple Storage Service (S3)

Infintitely Scaling Storage is one of the most popular AWS services.

# Buckets

Buckets (repositories)
Objects (files)

## Naming

Must have globally unique name defined at region level.

- No uppercase
- No underscore
- 63 char max
- not an IP address
- must start with a lowercase letter or a number

## Objects

Objects must have a key that is the full path. Example keys/paths:

  file.txt
  folder1/folder2/file.txt

The key can b broked down into prefix + object name:

  prefix = folder1/folder2/
  object name = file.txt

Objects are the contents of the files
- Max size is 5 TB
- Can't upload more than 5 GB.
- Files larger than 5GB must be uploaded as a multi-part upload

Objects can have **metadata** that is key/value pairs.

Objects can have **tags** for security or lifecycle management.

Objects can be versioned if enabled.

# Versioning

Objects can be versioned at the bucket level.

Every time the object is updated it will change. Hashes.
- Protected from overwrites
- Easy to roll back

Things to Remember:
- Any non versioned object will be null
- Suspending versioning will not delete previous versions.
- Objects that have been deleted have a delete marker

# Encryption

There are 4 methods of encrypting objects

SSE = Server Side Encryption

- SSE-S3 - Keys managed by S3
- SSE-KMS - Use Key Management Service to manage encryption keys
- SSE-C - Managed by the user
- Client Side Encryption

## SSE-S3

- Server-side encryption
- AES-256
- Must set header to:  x-amz-server-side-encryption: AES256

HTTPS + Header upload of object will encrypt into the bucket

## SSE-KMS

- Server-side encryption
- user control + audit trail
- Must set header to:  x-amz-server-side-encryption: aws:kms

## SSE-C

- Server-side encryption
- Amazon doesn't store the encryption key
- Must use HTTPS (mandatory)
- Encryption key must be in the headers

HTTPS + Data key in header. AWS doesn't persist key

## Client Side Encryption

Use client libraries suce has Amazon S3 Encryption Client

- Clients must encrypt/decrypt before sending/retrieving.
- Customer manages all of the keys

## SSL

Amazon S3 supports both HTTP and HTTPS endpoints

# S3 Security Policies

JSON based policies for access for buckets and objects

Policies include:
- Allow and Deny access
- Account or user to apply policy to

Used for:
- Public access to a bucket
- Force object to be encrypted
- Grant access

Settings for Block Public Access
- New ACLs
- Any ACLs
- New public bucket or access point

Networking
- Supports VPC Endpoints

Logging and Audit
- S3 Access logs can be stored in other S3 buckets
- API calls can be logged in AWS CloudTrail

User Security
- MFA required for delete in versioned buckets
- Expired URLs

# S3 Websites

Can host static websites. Can be accessible with

- < bucket-name >.s3-website-< region >.amazonaws.com
- < bucket-name >.s3-website.< region >.amazonaws.com

If you get a 403, that is a bucket policy that doesn't allow for public access.

# Cross Origin Region Sharing (CORS)

An origin is a protocol, domain, and port. If you need to access another origin and it only wants to deliver it from the first origin, you have to add to the header (Access-Control-Allow-Origin) and then it will provide it to the user. Otherwise it will error out.

S3 buckets works similarly because of S3 Websites. It is a way to block websites from referrring to your static assets unless there is a CORS policy

# Consistency Model

Any time you put or overwrite an object in a bucket, the next read will now be consistent for reading or listing. Thise didn't use to be the case.

# Storage Classes

- Standard - General Purpose
- Standard Infrequent Access (IA)
- One Zone-Infrequent Access
- Glacier Instant Retrieval
- Glacier Flexible Retrieval
- Glacier Deep Archive
- Intelligent Tiering

You use lifecycle configurations to move objects between classes

## Durability and Availability

Durability
- High durability. 11 9s (99.999999999%).
- If you store 10M object you can lose one object every 10,000 years
- Same for all storage classes

Availability
- Measures how readily available a service is
- Varies by storage class
- S3 is 99.99 which is not available 53 minutes a year

## S3 Standard

- 99.99 Availability
- Used for frequently accessed data
- Low latency and high throughput
- Sustain 2 concurrent facility failures
- Use cases: Big Data, mobile/gaming, content distribution

## Infrequent access

- Lower cost than standard
- Rapid access, but less frequently used

Standard
- 99.99 Availability
- Use cases: Disaster recovery/backups

One Zone
- Stored in one AZ, if AZ goes down **poof**
- 99.5 Availability
- Use cases: Secondory backups, or recreatable data

## Glacier

- For archiving/backup
- Price for storage + object retrieval cost

Glacier Instant Retrieval
- ms retrieval, great for data accessed once a quarter
- minimum storage for 90 days

Glacier Flexible Retrieval
- Expedited (1-5 minutes), Standard (3-5 hrs), Bulk (5-12 hours) free
- minimum storage for 90 days

Glacier Deep Archive
- Standard (12 hrs), Bulk (48 hours) free
- minimum storage for 180 days

## S3 Intelligent Tiering

- Small monthly monitoring fee
- Moves objects automatically  between tiers based on usage
- No retrieval charges

- Frequent tier (automatic): default
- Infrequent Access tier: objects not accessed for 30 days
- Archive Instant Access tier: objects not accessed for 90 days
- Archive Access: configurable from 90 days to 700+ days
- Deep Archive Access tier: configurable from 180 days to 700+ days

# Lifecycle Management

- For infrequently accessed objects use Standard IA
- For archive objects use glacier or deep archive

Transition Actions
- Move to IA after 60 days
- Move to Glacier after 180 days

Expiration Actions
- Delete logs after 1 year
- Delete old versions of files
- Delete incomplete multi-part uploads

Rules can be created for a specific prefix (folder/*)

# MFA Delete

Using MFA before doing an important S3 operation like delete. Two reasons to use:

- Turn off Versioning
- Delete an object version

Don't need it for anything else like

- Enable versioning
- List deleted versions

It can only be enabled/disabled by the bucket owner (root account). And can only be setup via CLI.

# Default Encryption and Bucket Policies

You can force encryption with bucket policies. The policy will refuse all API calls to PUT an object without the encryption headers.

The other way is to use the Default Encryption option.

Bucket policies are alway evaluated before default encryption

# Access Logs

For auditing, you will want to log all of your access to another bucket. The data can be looked at with analysis tools or you could use Amazon Athena.

- Don't log to the S3 bucket that you are monitoring.
- This creates a loop that will grow your bucket

# Replication

Enable Versioning in source and destination buckets

- Cross Region Replication (CRR)
- Same Region Replication (SRR)

Copying is asynch. Buckets can be in different accounts. IAM perms to S3 are important.

Why use CRR?
- Compliance
- Lower Latency
- Replication into a different account

Why use SRR?
- log aggregation
- Replication live data to test accounts

After activating, only **new objects** are replicated. You have to use S3 Batch Replication for older or object that didn't replicate.

For DELETE:
- Can optionally replicate delete markers
- Deletion with a version ID are not replicated (prevent bad actors)

You can't chain replication. It is not transitive. Items replicated from bucket 1 to bucket 2 are not replicated to bucket 3

# Pre-signed URLs

Only can be generated via CLI or SDK
- Downloads: Use CLI
- Uploads: Harder, use SDK

When you sign a URL, valid for 3600 seconds by default which is changeable via `--expires-in <time in seconds>`

The URL generated inherits the permissions of the person who generated the URL for Get/Put

Use Cases:
- Allow only auth'd users to download content
- Allow a big user list that changes to download content by dynamically creating the URL.
- Allow a user to upload a file to upload to a specific place

# Performance

S3 has high request rates and low latency (100-200ms)

- 3500 PUT/COPY/POST/DELETE requests/s per prefix
- 5500 GET/HEAD requests/s per prefix

A prefix is the path in the bucket not including the object name. There is no limit to the number of prefixes in a bucket. 

If you have 4 prefixes, they could get 22000 GET requests/s throughput

## KMS

If you encrypt with SSE-KMS there are KMS limits
- Upload calls GenerateDataKey API
- Download calls Decrypt API
- KMS has a reqion quota (5500, 10K, 30K requests/s depending on region)
- Quota increase can happen through the Service Quota Console
- You will only hit that quota if you have really high user demands.

## Improving Performance for Upload

Multi-part Upload
- Recommended for > 100 MB
- Must bed used for > 5 GB
- Uploads are parallelized

S3 Transfer Acceleration
- Send file to Edge and that will target an S3 bucket in a different region.
- Works with multi-part upload

## Improving Performance for Download

S3 Byte-Range Fetches 
- Parallelize GETs by asking for specific byte ranges
- If there is a failure, just request the byte range that failed
- Useful if you only want a file header

# S3 Select / Glacier Select

Use SQL to get data via server side filtering.

- Can only filter rows and columns (simple SQL)
- Less network traffic and CPU on client
- Can be used with CSV files!

# Event Notifications

Example Events
- S3:ObjectCreated
- S3:ObjectRemoved
- S3:ObjectRestore

You can filter by object name (eg *.png). Event notifications happen within seconds so you can post-process an event (eg generate thumbnails)

Amazon EventBridge processes events and you can send them up to 18 current desination sources like Lambda or SQS.
- Filtering happens with JSON (object name, metadata, size ).
- Multiple destinations for one event
- What you can do with EventBridge? Archive them, Replay them

# Athena

Analytics against S3 objects (serverless)

- Queries files with SQL
- Supports CSV, JSON, Avro, Parquet, ORC (uses Presto)
- Dump the analytics into QuickInsight for reporting

Cost: $5/TB

Used for:
- Business Intelligence
- VPC Flow Logs
- ELB Logs
- CloudTrail Logs

# Access Points

Ability to grant different perms to folders within buckets.

- Limit Access points to IAM users/groups
- Only 1 policy per access point, not many per bucket making it complex

# Object Lambda Access Point

Use case: Changing S3 data before it is retrieved.
- Lambda function is used to modify the data before it is downloaded
- Need to create both S3 Access Points and S3 Object Lambda Access Point
 
Use Cases:
- Redaction of data
- Convert data formats like JSON to YAML
- Resizing and watermarking pngs for specific users who download an image

